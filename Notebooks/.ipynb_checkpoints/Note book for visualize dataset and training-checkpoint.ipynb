{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab468fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6638b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efce11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from useful_functions import *\n",
    "device = 'cuda'\n",
    "# remember to set GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n",
    "config = yaml.load(open('configs/Pretrain_4m.yaml', 'r'), Loader=yaml.Loader)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ff464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augumentations\n",
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "pretrain_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(config['image_res'], scale=(0.2, 1.0),\n",
    "                                 interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    RandomAugment(2, 7, isPIL=True, augs=['Identity', 'AutoContrast', 'Equalize', 'Brightness', 'Sharpness',\n",
    "                                          'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate']),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# first zoom picture then do corp so that argumentated picture can capture more region\n",
    "train_transform_zoom_corp = transforms.Compose([\n",
    "    transforms.Resize((int(config['image_res'] * 1.5), int(config['image_res'] * 1.5)), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(config['image_res'],# scale=(0.5, 1.0),\n",
    "                                 #interpolation=InterpolationMode.BICUBIC\n",
    "                         ),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    RandomAugment(2, 7, isPIL=True, augs=['AutoContrast','Identity', 'Brightness', 'Sharpness']),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# original train-transform used by CCLM paper\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.CenterCrop(config['image_res'],# scale=(0.5, 1.0),\n",
    "                                 #interpolation=InterpolationMode.BICUBIC\n",
    "                         ),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    RandomAugment(2, 7, isPIL=True, augs=['AutoContrast','Identity', 'Brightness', 'Sharpness']),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((config['image_res'], config['image_res']), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68886da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "for i in range(num_gpus):\n",
    "    gpu_name = torch.cuda.get_device_name(i)\n",
    "    print(f\"GPU {i}: {gpu_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f702d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess dataset\n",
    "data_path = '/mnt/swordfish-pool2/ccu/amith-cache.pkl'\n",
    "with open(data_path, 'rb') as handle:\n",
    "    dataset = pickle.load(handle)\n",
    "\n",
    "# delete file without jpgs\n",
    "keys_to_remove = []\n",
    "for key in dataset.keys():\n",
    "    if dataset[key]['data_type'] !='video':\n",
    "        keys_to_remove.append(key)\n",
    "    elif dataset[key]['processed'] == False:\n",
    "        keys_to_remove.append(key)\n",
    "for key in keys_to_remove:\n",
    "    del dataset[key]\n",
    "\n",
    "# train val test split\n",
    "train_dataset = {}\n",
    "val_dataset = {}\n",
    "test_dataset = {}\n",
    "final_eval_dataset = {}\n",
    "for key in dataset.keys():\n",
    "    if 'INTERNAL_TRAIN' in dataset[key]['splits']:\n",
    "        train_dataset.update({key:dataset[key]})\n",
    "    if 'EVALUATION_LDC2023E07' in dataset[key]['splits']:\n",
    "        final_eval_dataset.update({key:dataset[key]})\n",
    "    if 'INTERNAL_VAL' in dataset[key]['splits']:\n",
    "        val_dataset.update({key:dataset[key]})\n",
    "    if 'INTERNAL_TEST' in dataset[key]['splits']:\n",
    "        test_dataset.update({key:dataset[key]})\n",
    "\n",
    "print(f'Inference batch size {16}')\n",
    "\n",
    "print(len(dataset), len(train_dataset), len(val_dataset), len(test_dataset), len(final_eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset_csv(dataset,transcribe_name,part = None, use_context = True):\n",
    "    '''\n",
    "    'part' is used if you want to seperate a whole dataset into two part\n",
    "    '''\n",
    "    column_file_id = []\n",
    "    column_start_time = []\n",
    "    column_end_time = []\n",
    "    column_start_frame = []\n",
    "    column_end_frame = []\n",
    "    column_text = []\n",
    "    column_label = []\n",
    "    file_ids = list(dataset.keys())\n",
    "    pass_file_num = 0\n",
    "\n",
    "    if not use_context:\n",
    "        for file_id in file_ids:\n",
    "            file_root_path = dataset[file_id]['processed_dir']\n",
    "            changepoint_list = [changepoint_dict['timestamp'] for changepoint_dict in dataset[file_id]['changepoints']]\n",
    "            for idx in range(len(dataset[file_id]['utterances'][transcribe_name])):\n",
    "                sample = dataset[file_id]['utterances'][transcribe_name][idx]\n",
    "                if len(sample['video_frames'])>=2:\n",
    "                    column_file_id.append(file_id)\n",
    "                    column_start_time.append(sample['start'])\n",
    "                    column_end_time.append(sample['end'])\n",
    "                    start_frame_path = os.path.join(file_root_path,sample['video_frames'][0][1])\n",
    "                    end_frame_path = os.path.join(file_root_path,sample['video_frames'][-1][1])\n",
    "                    column_start_frame.append(start_frame_path)\n",
    "                    column_end_frame.append(end_frame_path)\n",
    "                    if len(sample['text'])>512:\n",
    "                        column_text.append(sample['text'][:500])\n",
    "                    else:\n",
    "                        column_text.append(sample['text'])\n",
    "                    label = is_a_changepoint(changepoint_list, sample['start'],sample['end'])\n",
    "                    column_label.append(label)\n",
    "    else:\n",
    "        for file_id in file_ids:\n",
    "            file_root_path = dataset[file_id]['processed_dir']\n",
    "            changepoint_list = [changepoint_dict['timestamp'] for changepoint_dict in dataset[file_id]['changepoints']]\n",
    "            total_text = []\n",
    "            for idx in range(len(dataset[file_id]['utterances'][transcribe_name])):\n",
    "                sample = dataset[file_id]['utterances'][transcribe_name][idx]\n",
    "                if len(sample['text'])>512:\n",
    "                        total_text.append(sample['text'][:300])\n",
    "                else:\n",
    "                    total_text.append(sample['text'])\n",
    "            for idx in range(len(dataset[file_id]['utterances'][transcribe_name])):\n",
    "                sample = dataset[file_id]['utterances'][transcribe_name][idx]\n",
    "                total_text\n",
    "                if len(sample['video_frames'])>=2:\n",
    "                    column_file_id.append(file_id)\n",
    "                    column_start_time.append(sample['start'])\n",
    "                    column_end_time.append(sample['end'])\n",
    "                    start_frame_path = os.path.join(file_root_path,sample['video_frames'][0][1])\n",
    "                    end_frame_path = os.path.join(file_root_path,sample['video_frames'][-1][1])\n",
    "                    column_start_frame.append(start_frame_path)\n",
    "                    column_end_frame.append(end_frame_path)\n",
    "                    if idx>=10 and idx<=len(dataset[file_id]['utterances'][transcribe_name])-10:\n",
    "                        pre_context = ' '.join(total_text[idx-10:idx])\n",
    "                        cur_context = total_text[idx][:256]\n",
    "                        post_context = ' '.join(total_text[idx+1:idx+11])\n",
    "                        left_token_len = 400 - len(cur_context)\n",
    "                        pre_context = pre_context[max(0,(len(pre_context)-int(left_token_len/2))):]\n",
    "                        post_context = post_context[:int(left_token_len/2)]\n",
    "                        final_text = pre_context+'<Pre_Context>'+cur_context+'<Post_Context>'+post_context\n",
    "                        column_text.append(final_text)\n",
    "                    elif idx<10:\n",
    "                        pre_context = ' '.join(total_text[:idx])\n",
    "                        cur_context = total_text[idx][:256]\n",
    "                        post_context = ' '.join(total_text[idx+1:idx+11])\n",
    "                        left_token_len = 400 - len(cur_context)\n",
    "                        pre_context = pre_context[max(0,(len(pre_context)-int(left_token_len/2))):]\n",
    "                        post_context = post_context[:int(left_token_len/2)]\n",
    "                        final_text = pre_context+'<Pre_Context>'+cur_context+'<Post_Context>'+post_context\n",
    "                        column_text.append(final_text)\n",
    "                    elif idx>len(dataset[file_id]['utterances'][transcribe_name])-10:\n",
    "                        pre_context = ' '.join(total_text[idx-10:idx])\n",
    "                        cur_context = total_text[idx][:256]\n",
    "                        post_context = ' '.join(total_text[idx+1:])\n",
    "                        left_token_len = 400 - len(cur_context)\n",
    "\n",
    "                        pre_context = pre_context[max(0,(len(pre_context)-int(left_token_len/2))):]\n",
    "                        post_context = post_context[:int(left_token_len/2)]\n",
    "                        final_text = pre_context+'<Pre_Context>'+cur_context+'<Post_Context>'+post_context\n",
    "                        column_text.append(final_text)\n",
    "\n",
    "                    label = is_a_changepoint(changepoint_list, sample['start'],sample['end'])\n",
    "                    column_label.append(label)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'file_id':column_file_id,\n",
    "        'img_start':column_start_frame,\n",
    "        'img_end':column_end_frame,\n",
    "        'text':column_text,\n",
    "        'label':column_label,\n",
    "        'time_start':column_start_time,\n",
    "        'time_end':column_end_time\n",
    "    })\n",
    "    if part == 1:\n",
    "        print(int(len(df)/2))\n",
    "        df = df.iloc[:int(len(df)/2),:]\n",
    "    elif part == 2:\n",
    "        df = df.iloc[int(len(df)/2):,:]\n",
    "    elif part is None:\n",
    "        pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ab471",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = construct_dataset_csv(train_dataset,'whisper',use_context = True)\n",
    "#val_csv = construct_dataset_csv(val_dataset,'whisper',use_context = True)\n",
    "#test_csv = construct_dataset_csv(test_dataset,'whisper',use_context = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_parameters \n",
    "train_batch_size_for_test = 32\n",
    "test_seed = 123\n",
    "\n",
    "# val_dataset\n",
    "val_dataset = LDCDataset_val(val_csv, val_transform)\n",
    "\n",
    "\n",
    "# train_dataloader\n",
    "# random_corp dataloader\n",
    "test_data_loader = create_down_sample_dataloader(train_csv, 123, train_batch_size_for_test, \n",
    "                              train_transform,1)\n",
    "\n",
    "# zoom_corp dataloader\n",
    "#test_data_loader = create_down_sample_dataloader(train_csv, 123, train_batch_size_for_test, \n",
    "#                              train_transform_zoom_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03787bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_data_loader:\n",
    "    show_a_batch(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d46566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this chunk when training\n",
    "# parameters\n",
    "train_batch_size = 15\n",
    "eval_batch_size = 512\n",
    "num_epoch = 100\n",
    "custom_lr = 0.0002\n",
    "# original 0.0001\n",
    "\n",
    "# load model\n",
    "my_nvlr_model = NLVRModel(config=config)\n",
    "my_nvlr_model.load_pretrained('data/cclm_4m_epoch_29.th', config,  ## need to match 3m or 4m\n",
    "                              load_nlvr_pretrain= False, is_eval=False) # load_nlvr_pretrain= False because current checkpoint is CCLM model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "#special_tokens = ['<Pre_Context>', '<Post_Context>']\n",
    "#tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "#my_nvlr_model.text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# training parameters \n",
    "world_size = utils.get_world_size()\n",
    "\n",
    "arg_opt = utils.AttrDict(config['optimizer'])\n",
    "arg_opt['lr'] = custom_lr\n",
    "optimizer = create_optimizer(arg_opt, my_nvlr_model)\n",
    "arg_sche = utils.AttrDict(config['schedular'])\n",
    "arg_sche['step_per_epoch'] = math.ceil(len(train_dataset)/(train_batch_size*world_size))\n",
    "lr_scheduler = create_scheduler(arg_sche, optimizer)\n",
    "log = []\n",
    "my_nvlr_model = nn.DataParallel(my_nvlr_model)\n",
    "my_nvlr_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8803c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "metric_logger.add_meter('lr', utils.SmoothedValue(window_size=50, fmt='{value:.6f}'))\n",
    "metric_logger.add_meter('loss', utils.SmoothedValue(window_size=50, fmt='{value:.4f}'))\n",
    " \n",
    "print_freq = 6\n",
    "train_accuracys = []\n",
    "train_recalls = []\n",
    "train_precisions = []\n",
    "train_aucs = []\n",
    "train_auc_precisions = []\n",
    "train_auc_recalls = []\n",
    "val_accuracys = []\n",
    "val_recalls = []\n",
    "val_precisions = []\n",
    "val_aucs = []\n",
    "val_auc_precisions = []\n",
    "val_auc_recalls = []\n",
    "\n",
    "top_models = []\n",
    "# dirs\n",
    "#os.makedirs('/mnt/swordfish-pool2/kh3074/train_cls_head_first/evaluate_results')\n",
    "#os.makedirs('/mnt/swordfish-pool2/kh3074/train_cls_head_first/save_models')\n",
    "#os.makedirs('/mnt/swordfish-pool2/kh3074/train_cls_head_first/trained_cls_head_model')\n",
    "model_save_dir = '/mnt/swordfish-pool2/kh3074/train_cls_head_first/trained_cls_head_model'\n",
    "log_save_dir = '/mnt/swordfish-pool2/kh3074/train_cls_head_first/evaluate_results'\n",
    "\n",
    "\n",
    "print('Start training!!')\n",
    "for epoch in range(num_epoch):\n",
    "    # start new epoch of training\n",
    "    my_nvlr_model.train()\n",
    "    for param in my_nvlr_model.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in my_nvlr_model.module.cls_head.parameters():\n",
    "    #for param in my_nvlr_model.cls_head.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    train_data_loader = create_down_sample_dataloader(train_csv, epoch, train_batch_size, \n",
    "                              train_transform,1,evaluation = False) # use epoch as random seed\n",
    "    header = 'Train Epoch: [{}]'.format(epoch) \n",
    "    for i, (image0, image1, text, targets) in enumerate(metric_logger.log_every(train_data_loader, print_freq, header)):\n",
    "        images = torch.cat([image0, image1], dim=0)\n",
    "        images, targets = images.to(device), targets.to(device)   \n",
    "\n",
    "        text_inputs = tokenizer(text, padding='longest', return_tensors=\"pt\").to(device)  \n",
    "\n",
    "        loss = my_nvlr_model(images, text_inputs.input_ids, text_inputs.attention_mask, targets=targets, train=True)\n",
    "        loss = loss.mean() # aggregate loss from different GPUs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        \n",
    "    # start evaluation\n",
    "    my_nvlr_model.eval() \n",
    "    # create eval_Dataloader with fix seed for val dataset\n",
    "    print('start eval on train dataset ---------------------------------------------')\n",
    "    train_eval_data_loader = create_down_sample_dataloader(train_csv, 2333, eval_batch_size, \n",
    "                              val_transform,1,evaluation = True)\n",
    "    eval_train_df = eval_on_dataset(my_nvlr_model,train_eval_data_loader,device,tokenizer)\n",
    "    train_recall, train_precision, train_accuracy, train_pr_auc, train_precision_auc, train_recall_auc = calculate_matrix(eval_train_df)\n",
    "    \n",
    "    train_accuracys.append(train_accuracy)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_precisions.append(train_precision)\n",
    "    train_aucs.append(train_pr_auc)\n",
    "    train_auc_precisions.append(train_precision_auc)\n",
    "    train_auc_recalls.append(train_recall_auc)\n",
    "    \n",
    "    \n",
    "    print('start eval on validation dataset ---------------------------------------------')\n",
    "    val_data_loader = create_down_sample_dataloader(val_csv, 2333, eval_batch_size, \n",
    "                              val_transform,1,evaluation = True) # fix seed 2333\n",
    "    eval_val_df = eval_on_dataset(my_nvlr_model,val_data_loader,device,tokenizer)\n",
    "    val_recall, val_precision, val_accuracy, val_pr_auc, val_precision_auc, val_recall_auc = calculate_matrix(eval_val_df)\n",
    "    \n",
    "    val_accuracys.append(val_accuracy)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_precisions.append(val_precision)\n",
    "    val_aucs.append(val_pr_auc)\n",
    "    val_auc_precisions.append(val_precision_auc)\n",
    "    val_auc_recalls.append(val_recall_auc)\n",
    "    \n",
    "    \n",
    "    # if auc of val dataset is the top 3, then save the model\n",
    "    if len(top_models) < 3:\n",
    "        top_models.append((f'model_tuned_epoch_{epoch}', val_pr_auc))\n",
    "    else:\n",
    "        top_models.append((f'model_tuned_epoch_{epoch}', val_pr_auc))\n",
    "        top_models.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if os.path.exists(os.path.join(model_save_dir,top_models[-1][0])):\n",
    "            os.remove(os.path.join(model_save_dir,top_models[-1][0]))\n",
    "        top_models.pop(3)\n",
    "        for i in range(3):\n",
    "            if os.path.exists(os.path.join(model_save_dir,top_models[i][0])):\n",
    "                pass\n",
    "            else:\n",
    "                torch.save(my_nvlr_model.state_dict(), os.path.join(model_save_dir,top_models[i][0]))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger.global_avg())     \n",
    "    log.append({k: \"{:.5f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()})\n",
    "    \n",
    "    #vis\n",
    "    train_result_df = pd.DataFrame({'train_accuracys':train_accuracys,\n",
    "                         'train_recalls':train_recalls ,\n",
    "                         'train_precisions':train_precisions ,\n",
    "                         'train_aucs':train_aucs ,\n",
    "                         'train_auc_precisions':train_auc_precisions ,\n",
    "                         'train_auc_recalls':train_auc_recalls ,\n",
    "                         'val_accuracys':val_accuracys ,\n",
    "                         'val_recalls':val_recalls ,\n",
    "                         'val_precisions':val_precisions ,\n",
    "                         'val_aucs':val_aucs ,\n",
    "                         'val_auc_precisions':val_auc_precisions ,\n",
    "                         'val_auc_recalls':val_auc_recalls })\n",
    "    \n",
    "    fig, ax = plt.subplots(1,4, figsize = (15,5))\n",
    "    epoch_idx = range(len(train_result_df))\n",
    "\n",
    "    ax[0].plot(epoch_idx,train_result_df.train_accuracys)\n",
    "    ax[0].set_title('accuracy')\n",
    "\n",
    "    ax[1].plot(epoch_idx,train_result_df.train_recalls)\n",
    "    ax[1].set_title('recall')\n",
    "\n",
    "    ax[2].plot(epoch_idx,train_result_df.train_precisions)\n",
    "    ax[2].set_title('precision')\n",
    "\n",
    "    ax[3].plot(epoch_idx,train_result_df.train_aucs)\n",
    "    ax[3].set_title('auc')\n",
    "\n",
    "\n",
    "    ax[0].plot(epoch_idx,train_result_df.val_accuracys)\n",
    "    ax[1].plot(epoch_idx,train_result_df.val_recalls)\n",
    "    ax[2].plot(epoch_idx,train_result_df.val_precisions)\n",
    "    ax[3].plot(epoch_idx,train_result_df.val_aucs)\n",
    "    plt.show()\n",
    "    \n",
    "    train_result_df.to_csv(os.path.join(log_save_dir,'train_transform_zoom_corp'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": "20",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
